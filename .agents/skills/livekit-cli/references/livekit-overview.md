# LiveKit overview

LiveKit is a realtime communication platform for building AI-native applications with audio, video, and data streaming. This overview helps you understand the LiveKit ecosystem and how to use these skills effectively.

## Platform components

### LiveKit Cloud

LiveKit Cloud is a fully managed platform for building, deploying, and operating AI agent applications. It includes:

- **Realtime media infrastructure** - Global mesh of servers for low-latency audio, video, and data streaming
- **Managed agent hosting** - Deploy agents without managing servers or orchestration
- **LiveKit Inference** - Run AI models directly within LiveKit Cloud without API keys
- **Native telephony** - Provision phone numbers and connect PSTN calls directly to rooms
- **Observability** - Built-in analytics, logs, and quality metrics

### Agents framework

The Agents framework lets you build Python or Node.js programs that join LiveKit rooms as realtime participants. Key capabilities:

- **Voice pipelines** - Stream audio through STT-LLM-TTS pipelines
- **Realtime models** - Use models like OpenAI Realtime API that handle speech directly
- **Tool calling** - Define functions the LLM can invoke during conversations
- **Multi-agent workflows** - Hand off between specialized agents
- **Turn detection** - State-of-the-art model for natural conversation flow

### Architecture

```
┌─────────────┐     WebRTC      ┌─────────────┐     HTTP/WS      ┌─────────────┐
│   Frontend  │ ◄─────────────► │   LiveKit   │ ◄──────────────► │    Agent    │
│  (Web/App)  │                 │    Room     │                  │   Server    │
└─────────────┘                 └─────────────┘                  └─────────────┘
                                       │                                │
                                       │                                │
                                       ▼                                ▼
                                ┌─────────────┐                  ┌─────────────┐
                                │  Telephony  │                  │  AI Models  │
                                │    (SIP)    │                  │ (STT/LLM/TTS)│
                                └─────────────┘                  └─────────────┘
```

## How these skills work together

The LiveKit skills cover the full stack for building voice AI applications:

| Skill | Purpose | Language |
|-------|---------|----------|
| `agents-py` | Build agent backends | Python |
| `agents-ts` | Build agent backends | TypeScript/Node.js |
| `agents-ui` | Build agent frontends | React |

**Typical workflow:**

1. **Choose your backend** - Use `agents-py` or `agents-ts` based on your team's preference
2. **Build the frontend** - Use `agents-ui` for React-based web interfaces
3. **Connect via LiveKit** - Both connect to the same LiveKit room for realtime communication

## Using the skills effectively

### When to use each skill

- **Building a new voice agent?** Start with `agents-py` or `agents-ts` for the backend logic
- **Need a web interface?** Add `agents-ui` for pre-built React components
- **Full-stack project?** Use both a backend skill and `agents-ui` together

### Combining skills

The skills are designed to work together. A typical project structure:

```
my-voice-app/
├── agent/           # Use agents-py or agents-ts skill
│   └── agent.py     # or agent.ts
├── frontend/        # Use agents-ui skill
│   └── src/
│       └── app/
└── .env.local       # Shared LiveKit credentials
```

### Environment setup

All skills require LiveKit credentials:

```bash
LIVEKIT_API_KEY=your_api_key
LIVEKIT_API_SECRET=your_api_secret
LIVEKIT_URL=wss://your-project.livekit.cloud
```

Get these from your [LiveKit Cloud dashboard](https://cloud.livekit.io) or self-hosted deployment.

## Resources

- [LiveKit documentation](https://docs.livekit.io)
- [Agents framework guide](https://docs.livekit.io/agents)
- [LiveKit Cloud](https://cloud.livekit.io)
- [GitHub repositories](https://github.com/livekit)
